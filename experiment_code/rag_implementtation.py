# -*- coding: utf-8 -*-
"""RAG - implementtation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iGjL1wI-NR5526OIf4G4Ws3ne8HZpMgq
"""

!pip install openai
!pip install langchain_community
!pip install langchain
!pip install python-docx
!pip install chromadb
!pip install tiktoken
!pip install langchain_openai
! pip install langchain_chroma

# from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.messages import HumanMessage
from langchain.chains import create_retrieval_chain
from langchain.schema import Document as LangchainDocument
# from typing import Dict
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.messages import AIMessage, HumanMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnableBranch

import os
from docx import Document
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import SequentialChain
from langchain.chains import LLMChain
from langchain.chains import SimpleSequentialChain

llm_model = "gpt-3.5-turbo"
OPENAI_API_KEY = "XXX"
llm = ChatOpenAI(temperature=0.9, model=llm_model, openai_api_key=OPENAI_API_KEY)

# Load documents from a directory
def load_docx_from_dir(directory):
    all_document_text = []
    for filename in os.listdir(directory):
        if filename.endswith(".docx"):
            document_path = os.path.join(directory, filename)
            each_document = Document(document_path)
            texts = [para.text for para in each_document.paragraphs if para.text.strip()]
            all_document_text.extend(texts)
    return all_document_text

document_direction = "./"
document_texts = load_docx_from_dir(document_direction)

# Preprocess the text to tag dentist questions and patient answers
tagged_texts = []
for text in document_texts:
    if "Dentist:" in text or "Patient:" in text:
        tagged_texts.append(text)

# Wrap text in LangchainDocument objects
documents = [LangchainDocument(page_content=text) for text in tagged_texts]

# Split the documents into manageable chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150, add_start_index=True)
split_documents = text_splitter.split_documents(documents)
for i, doc in enumerate(split_documents):
    print(f"Chunk {i+1}: {doc.page_content}\n")
#
# Create embeddings for the documents
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
db = Chroma.from_documents(documents=split_documents, embedding=embeddings)
retriever = db.as_retriever()

search_greeting_prompt = (
        """
        You are a highly skilled retriever tasked with searching for answers related to the user's input question.
        Retrieve the question closest to the current user input, then get the corresponding answer to that question.
        Use the following pieces of retrieved context to create a new answer through imitation.
        Ensure the context retrieved includes both the closest question and its following answer.
        ###
        {context}
        ###
        The output should include only the answer to the retrieved question.
        If you have the retrieved context, please mimic its answer to create a new one.
        If no relevant documents are found, only generate sentences where a patient greets a dentist, responding as the patient.
        """
    )
greeting_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", search_greeting_prompt),
        ("human", "{input}")
    ]
)

greeting_prompt_chain = create_stuff_documents_chain(llm, greeting_prompt)
# print(f'greeting_prompt_chain:{greeting_prompt_chain}')
rag_greeting_prompt_chain = create_retrieval_chain(retriever, greeting_prompt_chain)
# print(f'rag_greeting_prompt_chain: {rag_greeting_prompt_chain}')
greeting_answer = rag_greeting_prompt_chain.invoke({"input": "I am good, how about you? Do you have a nice day?"})
print(greeting_answer)
print(greeting_answer['answer'])

conversation="Hello Dr, I'm here for my routine dental check-up. I have been experiencing tooth pain and my teeth have been feeling sensitive lately. I brush my teeth twice a day with a soft bristle toothbrush. I don't have any allergies, and I don't smoke or drink alcohol. The pain is located on the lower left side of my mouth. It occurs sporadically but tends to worsen when I consume hot or cold foods. The tooth pain started about a week ago. No, I am not taking any pain killers for the tooth pain. Yes, I try to maintain a healthy diet by incorporating plenty of fruits, vegetables, and whole grains into my meals. I also make sure to drink plenty of water throughout the day to stay hydrated."
patient_information="""- Age: 45
- Gender: female
- Symptoms: tooth pain, sensitive teeth
- Allergy history: no allergy
- Social habits:
    - Smokes: false
    - Drinks alcohol: false
- Lifestyle habits:
    - Likes to eat sweets: true
    - Tooth brushing habits: twice a day
    - Tooth brushing method: soft bristle toothbrush"""
dentist_question='Are you  consuming sugary snacks between meals?'
emotion='stressful'
scenario='Dntal Clinical Scenario_Bleeding Gums'

first_prompt = ChatPromptTemplate.from_template(
    """
    Based on the conversation context, summarize the key points in a known_message.
    Conversation:
    ###
    {conversation}
    ###
    Generate a summary message as known_message.
    """
)
# Chain 1: input=conversation, output=known_message
chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key="known_message")

known_message = chain_one.run({"conversation": conversation})
print(f'known_message is {known_message}')

second_prompt = ChatPromptTemplate.from_template(
    """
    You are the patient who is going to see a dentist. what you response should base on your personality. The conversation will base on scenario. If the message has been told (known_message) to the dentist,
    and when the dentist repeate again the question, you should explain in detail for your previous response.
    If the dentist ask whether you have any question, you should ask the patient's question about what you concern about or anything related to the conversation.
    Only respond to the dentist's question without including any unrelated content (in several sentences).
    The entire conversation should revolve around inquiring about detailed patient information before performing any actual dental diagnostic procedures.
    The generated dialogue should be coherent and natural, with seamless transitions.
    As the patient visiting a dentist, follow the scenario below to answer the dentist's question in a few sentences from the patient's perspective.
    It should generate only several sentences and wait for the dentist to respond.

    The information of you is:
    ###
    {patient_information}
    ###
    The message that you have already told dentist is:
    ###
    {known_message}
    ###
    The dentist's question is:
    ###
    {dentist_question}
    The personality of you is:
    ###
    {emotion}
    ###
    The scenario of you is:
    ###
    {scenario}
    ###

    Remember to keep your response relevant to the dentist's question from the patient's perspective.
    """
)
# chain 2
chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key="answer")
# Create the SequentialChain
overall_chain = SequentialChain(
    chains=[chain_one, chain_two],
    input_variables=["conversation", "patient_information", "dentist_question","emotion","scenario"],
    output_variables=["answer"],
    verbose=True
)
# Generate the patient conversation
response = overall_chain.run({
    "conversation": conversation,
    "patient_information": patient_information,
    "dentist_question": dentist_question,
    "emotion": emotion,
    "scenario": scenario
})
print(f'The overall chain is :{overall_chain}')
print(response)

search_medical_information_prompt = (
        """
        You are a highly skilled retriever tasked with searching for the most relevant context of the dentist's input in a conversation.
        Retrieve the most relevant conversation snippet related to the current user input.
        Extract the two sentences before and the two sentences after the user's input to provide the full context.
        ###
        {context}
        ###
        The output of answer should include only the two sentences before the user's input, the user's input itself, and the two sentences after the user's input, in that order.
        The output should include both dentist and patient's response in the document.
        If no relevant documents are found, explain that no relevant context could be retrieved.

        """
    )
medical_information_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", search_medical_information_prompt),
        ("human", "{input}")
    ]
)

medical_information_prompt_chain = create_stuff_documents_chain(llm, medical_information_prompt)
rag_medical_information_chain = create_retrieval_chain(retriever, medical_information_prompt_chain)
medical_information_answer = rag_medical_information_chain.invoke({"input": "That sounds manageable. How long does it usually take to see improvement?"})
print(medical_information_answer)
print(medical_information_answer['answer'])