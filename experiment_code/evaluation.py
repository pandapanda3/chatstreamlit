# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QeDLAXkZ0ELvMD42GUMW0cIoqSLWcykE
"""

!pip install rouge-score bert-score
!pip install rouge
!pip install python-docx
!pip install openai
!pip install langchain_community
!pip install langchain
!pip install python-docx
!pip install chromadb
!pip install tiktoken
!pip install langchain_openai
! pip install langchain_chroma

from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction
from rouge import Rouge
from bert_score import score as bert_score

# compute token and money
from langchain_community.callbacks.manager import get_openai_callback

"""# get answer from model (candidate)"""

# from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.messages import HumanMessage
from langchain.chains import create_retrieval_chain
from langchain.schema import Document as LangchainDocument
# from typing import Dict
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.messages import AIMessage, HumanMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnableBranch

import os
from docx import Document
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import SequentialChain
from langchain.chains import LLMChain
from langchain.chains import SimpleSequentialChain

openai_api_key = "XXX"

"""## RAG"""

# Load documents from a directory
def load_docx_from_dir(directory):
    all_document_text = []
    for filename in os.listdir(directory):
        if filename.endswith(".docx"):
            document_path = os.path.join(directory, filename)
            each_document = Document(document_path)
            print(f'each_document:{each_document}')
            texts = [para.text for para in each_document.paragraphs if para.text.strip()]
            all_document_text.extend(texts)
    return all_document_text

def store_data(OPENAI_API_KEY):

    document_direction = "./"
    document_texts = load_docx_from_dir(document_direction)

    # Preprocess the text to tag dentist questions and patient answers
    tagged_texts = []
    for text in document_texts:
        if "Dentist:" in text or "Patient:" in text:
            tagged_texts.append(text)

    # Wrap text in LangchainDocument objects
    documents = [LangchainDocument(page_content=text) for text in tagged_texts]

    # Split the documents into manageable chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150, add_start_index=True)
    split_documents = text_splitter.split_documents(documents)
    # for i, doc in enumerate(split_documents):
    #     print(f"Chunk {i+1}: {doc.page_content}\n")
    #
    # Create embeddings for the documents
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    db = Chroma.from_documents(documents=split_documents, embedding=embeddings)
    retriever = db.as_retriever()
    print(f'Finish store_data : {retriever}')
    return retriever

"""# Get the answer"""

# using the same model
def define_model(openai_api_key=""):
    llm_model = "gpt-4-turbo"

    llm = ChatOpenAI(temperature=0.8, model=llm_model, openai_api_key=openai_api_key)
    return llm

# retrive information from document
def retrive_document(prompt, dentist_input, openai_api_key=""):
    llm = define_model(openai_api_key)
    retriever = store_data(openai_api_key)
    document_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", prompt),
            ("human", "{input}")
        ]
    )

    document_prompt_chain = create_stuff_documents_chain(llm, document_prompt)
    rag_document_prompt_chain = create_retrieval_chain(retriever, document_prompt_chain)
    document_answer = rag_document_prompt_chain.invoke({"input": dentist_input})
    answer = document_answer['answer']
    print(f'After retrive the document, the answer is {answer}')
    return answer

# get the relative document base on conversation
def retrive_dentist_patient_document(dentist_input, openai_api_key=""):
    search_medical_information_prompt = (
        """
        You are a highly skilled retriever tasked with searching for the most relevant context of the dentist's input in a conversation.
        Retrieve the most relevant conversation snippet related to the current user input.
        Extract the two sentences before and the two sentences after the user's input to provide the full context.
        ###
        {context}
        ###
        The output of answer should include only the two sentences before the user's input, the user's input itself, and the two sentences after the user's input, in that order.
        The output should include both dentist and patient's response in the document.
        If no relevant documents are found, explain that no relevant context could be retrieved.

        """
    )
    answer = retrive_document(search_medical_information_prompt, dentist_input, openai_api_key=openai_api_key)
    return answer

def generate_patient_conversation(patient_information, dentist_question, scenario, emotion, conversation="", openai_api_key=""):

    llm = define_model(openai_api_key)
    first_prompt = ChatPromptTemplate.from_template(
        """
        Based on the conversation context, summarize the key points in a known_message.
        Conversation:
        {conversation}
        Generate a summary message as known_message.
        """
    )
    # Chain 1: input=conversation, output=known_message
    chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key="known_message")

    # get example from document
    example=retrive_dentist_patient_document(dentist_question, openai_api_key=openai_api_key)
    second_prompt = ChatPromptTemplate.from_template(
        """
        You are the patient who is going to see a dentist. what you response should base on your personality.
        The conversation will base on scenario. If the message has been told (known_message) to the dentist,
        and when the dentist repeat again the question, you should explain in detail for your previous response.
        When the dentist asks if you have any questions, you might want to inquire about the treatment plan,
        including the duration, specific procedures, and pain management options. Additionally, ask about post-procedure care,
        such as recovery time and any dietary or activity restrictions. Finally, discuss preventive measures,
        follow-up arrangements, and cost and insurance coverage.
        Only respond to the dentist's question without including any unrelated content (in several sentences).
        The entire conversation should revolve around inquiring about detailed patient information before performing any actual dental diagnostic procedures.
        The generated dialogue should be coherent and natural, with seamless transitions.
        As the patient visiting a dentist, follow the scenario below to answer the dentist's question in a few sentences from the patient's perspective.
        It should generate only several sentences and wait for the dentist to respond.
        The answer should remove "Patient:"

        The information of you is:
        ###
        {patient_information}
        ###
        The message that you have already told dentist is:
        ###
        {known_message}
        ###
        The dentist's question is:
        ###
        {dentist_question}
        The personality of you is:
        ###
        {emotion}
        ###
        The scenario of you is:
        ###
        {scenario}
        ###
        The example is:
        ###
        {example}
        ###

        Remember to keep your response relevant to the dentist's question from the patient's perspective.
        """
    )
    # chain 2
    chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key="answer")
    # Create the SequentialChain
    overall_chain = SequentialChain(
        chains=[chain_one, chain_two],
        input_variables=["conversation", "patient_information", "dentist_question","emotion","scenario","example"],
        output_variables=["answer"],
        verbose=True
    )
    # Generate the patient conversation
    response = overall_chain.run({
        "conversation": conversation,
        "patient_information": patient_information,
        "dentist_question": dentist_question,
        "emotion": emotion,
        "scenario": scenario,
        "example":example
    })
    # print(f'The overall chain is :{overall_chain}')

    return response

"""# Compute the score"""

# the the second conversation from Case Scenarios-Breaking Bad News.docx
import os
from docx import Document
# Load documents from a directory
def load_conversation_from_dir(directory):
    all_document_text = []
    for filename in os.listdir(directory):
        if filename.endswith("Scenario 2.docx"):
            document_path = os.path.join(directory, filename)
            each_document = Document(document_path)
            texts = [para.text for para in each_document.paragraphs if para.text.strip()]
            all_document_text.extend(texts)
    return all_document_text

document_direction = "./"
conversation_document_texts = load_conversation_from_dir(document_direction)

# Preprocess the text to tag dentist questions and patient answers
conversation_texts = []
for text in conversation_document_texts:
    if "Dentist:" in text or "Patient:" in text:
        conversation_texts.append(text)
print(conversation_texts)

patient_information="""This patient, Patient is 55 years old male working at a construction site. He recently remarried and have 3 (grown up) children, with his previous wife. He attended the oral medicine department a week-ago after referral from their GDP. The GDP had noticed an area of erythroplakia (red patch) on the right buccal mucosa.
A biopsy has been performed and this confirms grade 3 squamous cell carcinoma (tumour/cancer). The lesion is bigger than 4cm (T3) but there is no lymph node involvement apparent on physical examination. The patient will require further tests to explore whether there are any secondary sites.
The patient smokes approximately 20 cigarettes a day (rolls own) and reports drinking ‘most days but not to the point of getting drunk’.
The patient has re-attended today to be informed of the results of the biopsy."""
emotion="""stressful"""
scenario="Case Scenarios-Breaking Bad News"

# conversation_texts=['Dentist: And how have you been feeling?', "Patient: Umm! I’ve been quite anxious since the last visit. I mean… “Biopsy” sounds scary though…. I hope it's nothing serious (nervously).", 'Dentist: Has someone come with you? I mean you can call the attendant in.', 'Patient: No, I am alone today.']
# compute the score
import textwrap
import time
rouge = Rouge()
bleu_score_list=[]
rouge_score_list=[]
bert_score_P_list=[]
bert_score_R_list=[]
bert_score_F1_list=[]
final_result_list = []
talked_conversation = ""
start_time = time.time()
grouped_conversations = [conversation_texts[i:i + 2] for i in range(0, len(conversation_texts), 2)]
with get_openai_callback() as Resource_Usage:
  for each_group in grouped_conversations:
      if len(each_group) != 2:
          continue

      dentist_question = each_group[0].replace("Dentist:", "").strip()
      print(f"dentist_question: {dentist_question}")
      patient_reference = each_group[1].replace("Patient:", "").strip()
      print(f"patient_reference: {patient_reference}, the patient_reference.split() is {patient_reference.split()}")
      # generate a response to the dentist's question
      patient_candidate=generate_patient_conversation(patient_information, dentist_question, scenario, emotion, conversation=talked_conversation, openai_api_key=openai_api_key)

      print(f"patient_candidate: {patient_candidate}")
      talked_conversation= "\n".join(patient_candidate)
      # get blue score
      bleu_score = sentence_bleu([patient_reference.split()], patient_candidate.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1, auto_reweigh=True)
      bleu_score=round(bleu_score, 4)
      print(f"BLEU-4: {bleu_score}")
      bleu_score_list.append(bleu_score)

      # get rouge score
      rouge_score = rouge.get_scores(patient_candidate, patient_reference, avg=True)['rouge-l']['f']
      rouge_score=round(rouge_score, 4)
      print(f"ROUGE-L: {rouge_score}")
      rouge_score_list.append(rouge_score)
      # get bert score
      P, R, F1 = bert_score([patient_candidate], [patient_reference], lang='en')
      print(f"BERTScore: P={P.mean()}, R={R.mean()}, F1={F1.mean()}")

      bert_score_P=round(P.mean().item(), 4)
      bert_score_R=round(R.mean().item(), 4)
      bert_score_F1=round(F1.mean().item(), 4)
      bert_score_P_list.append(bert_score_P)
      bert_score_R_list.append(bert_score_R)
      bert_score_F1_list.append(bert_score_F1)
      # get overall message
      final_result_list.append([
        dentist_question,
        patient_reference,
        patient_candidate,
        bleu_score,
        rouge_score,
        bert_score_P,
        bert_score_R,
        bert_score_F1,
    ])
print(f'The cost is : {Resource_Usage}')
end_time = time.time()
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")

print(bleu_score_list)
print(rouge_score_list)
print(final_result_list)

"""# Output the report"""

!pip install pandas openpyxl

import pandas as pd
df = pd.DataFrame(final_result_list, columns=[
    'Dentist Question', 'Patient Reference', 'Patient Candidate', 'BLEU Score', 'ROUGE Score',
    'BERT Score P', 'BERT Score R', 'BERT Score F1'
])
df.to_excel('gpt-4-turbo.xlsx', index=True)

!pip install numpy

import numpy as np
average_bleu_score =  round(np.mean(bleu_score_list), 4)
average_rouge_score = round(np.mean(rouge_score_list), 4)
average_bert_score_P = round(np.mean(bert_score_P_list), 4)
average_bert_score_R = round(np.mean(bert_score_R_list), 4)
average_bert_score_F1 = round(np.mean(bert_score_F1_list), 4)
print(f'Average BLEU Score: {average_bleu_score}')
print(f'Average ROUGE Score: {average_rouge_score}')
print(f'Average BERT Score P: {average_bert_score_P}')
print(f'Average BERT Score R: {average_bert_score_R}')
print(f'Average BERT Score F1: {average_bert_score_F1}')